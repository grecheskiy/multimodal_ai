{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import CocoCaptions\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Устройство для выполнения вычислений\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Загрузим модель CLIP и инициализируем tokenizer\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "model = model.float().train()\n",
    "tokenizer = clip.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оптимизатор и функция потерь\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6, weight_decay=0.01) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавлен Dropout в модель\n",
    "dropout_rate = 0.2  # Можно настроить этот параметр\n",
    "model.visual.dropout = nn.Dropout(dropout_rate)  # Применяется к визуальной ветви модели\n",
    "model.transformer.dropout = nn.Dropout(dropout_rate)  # Применяется к текстовой ветви модели\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Аугментация и нормализация\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),  # Преобразование в тензор\n",
    "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "    transforms.Lambda(lambda x: torch.clamp(x, min=0, max=1)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDataset(Dataset):\n",
    "    def __init__(self, root, annFile, transform=None, max_samples=None):\n",
    "        super().__init__()\n",
    "        self.coco = CocoCaptions(root=root, annFile=annFile)\n",
    "        self.transform = transform  # Сохраняем transform\n",
    "        self.max_samples = max_samples  # Максимальное количество данных для загрузки\n",
    "\n",
    "        # Если указано max_samples, ограничиваем количество данных\n",
    "        if self.max_samples is not None:\n",
    "            self.indices = list(range(min(len(self.coco), self.max_samples)))\n",
    "        else:\n",
    "            self.indices = list(range(len(self.coco)))  # Иначе используем весь датасет\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)  # Возвращаем количество выбранных данных\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Используем индекс из self.indices\n",
    "        actual_idx = self.indices[idx]\n",
    "        image, captions = self.coco[actual_idx]\n",
    "        caption = captions[0]  # Используем первое описание\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)  # Применяем transform\n",
    "        return image, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузчик данных\n",
    "def collate_fn(batch):\n",
    "    images, captions = [], []\n",
    "    for item in batch:\n",
    "        images.append(item[0].unsqueeze(0))  # Добавляем изображение\n",
    "        captions.append(item[1])  # Добавляем текст\n",
    "    images = torch.cat(images, dim=0)  # Объединяем изображения в батч\n",
    "    captions = tokenizer(captions, truncate=True).to(device)  # Токенизируем текст\n",
    "    return images, captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.65s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Создание DataLoader\n",
    "test_dataset = CocoDataset(\n",
    "    root=\"coco2017/train2017\",\n",
    "    annFile=\"coco2017/annotations/captions_train2017.json\",\n",
    "    transform=transform,\n",
    "    max_samples=5000  # Ограничиваем датасет 1000 примерами\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер батча изображений: torch.Size([64, 3, 224, 224])\n",
      "Количество текстовых описаний: 64\n"
     ]
    }
   ],
   "source": [
    "# Проверка работы DataLoader\n",
    "for images, captions in test_dataloader:\n",
    "    print(f\"Размер батча изображений: {images.shape}\")  # Ожидается [64, 3, 224, 224]\n",
    "    print(f\"Количество текстовых описаний: {len(captions)}\")  # Ожидается 64\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for images, captions in test_dataloader:\n",
    "#     if torch.isnan(images).any() or torch.isinf(images).any():\n",
    "#         print(\"Обнаружены некорректные значения в изображениях.\")\n",
    "#     if isinstance(captions, torch.Tensor) and (torch.isnan(captions).any() or torch.isinf(captions).any()):\n",
    "#         print(\"Обнаружены некорректные значения в текстах.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]: 100%|██████████| 79/79 [03:57<00:00,  3.00s/it, loss=21.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Average Loss: 73.038\n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/5]: 100%|██████████| 79/79 [03:56<00:00,  3.00s/it, loss=7.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Average Loss: 31.254\n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/5]: 100%|██████████| 79/79 [03:56<00:00,  2.99s/it, loss=0]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Average Loss: 20.634\n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/5]: 100%|██████████| 79/79 [03:56<00:00,  3.00s/it, loss=8.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Average Loss: 15.526\n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/5]: 100%|██████████| 79/79 [03:55<00:00,  2.98s/it, loss=3.68]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Average Loss: 11.813\n",
      "Model Saved.\n"
     ]
    }
   ],
   "source": [
    "# Обучение модели\n",
    "epochs = 5  # Количество эпох\n",
    "best_loss = np.inf  # Лучший loss\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0  # Для накопления loss за эпоху\n",
    "    model.train()  # Переводим модель в режим обучения\n",
    "\n",
    "    with tqdm(\n",
    "        enumerate(test_dataloader, 0),\n",
    "        total=len(test_dataloader),\n",
    "        desc=f\"Epoch [{epoch+1}/{epochs}]\",\n",
    "    ) as tepoch:\n",
    "        for i, (images, captions) in tepoch:\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            # Обнуляем градиенты\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Получаем эмбеддинги изображений и текста\n",
    "            image_features = model.encode_image(images)\n",
    "            text_features = model.encode_text(captions)\n",
    "\n",
    "            # Вычисляем loss (контрастный loss)\n",
    "            logits_per_image = (image_features @ text_features.T) * model.logit_scale.exp()\n",
    "            logits_per_text = logits_per_image.t()\n",
    "\n",
    "            # Создаем метки (диагональ — правильные пары)\n",
    "            labels = torch.arange(len(images), device=device)\n",
    "\n",
    "            # Вычисляем loss для изображений и текста\n",
    "            loss_image = torch.nn.functional.cross_entropy(logits_per_image, labels)\n",
    "            loss_text = torch.nn.functional.cross_entropy(logits_per_text, labels)\n",
    "            loss = (loss_image + loss_text) / 2\n",
    "\n",
    "            # Сохраняем веса до обновления\n",
    "            # weights_before = [param.clone() for param in model.parameters()]\n",
    "\n",
    "            # Обратное распространение и обновление весов\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Проверка обновления весов\n",
    "            # weights_after = list(model.parameters())\n",
    "            # updated = any(not torch.allclose(before, after) for before, after in zip(weights_before, weights_after))\n",
    "            # print(\"Весы обновлены:\", updated)\n",
    "\n",
    "            # Проверка градиентов\n",
    "            # has_gradients = any(param.grad is not None and torch.any(param.grad != 0) for param in model.parameters())\n",
    "            # print(\"Градиенты вычислены и не равны нулю:\", has_gradients)\n",
    "\n",
    "            # Обновляем progress bar\n",
    "            tepoch.set_postfix(loss=loss.item())\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    # Средний loss за эпоху\n",
    "    avg_loss = epoch_loss / len(test_dataloader)\n",
    "\n",
    "    # Вывод итогового результата эпохи\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Average Loss: {avg_loss:.3f}\")\n",
    "    \n",
    "    # Сохраняем модель, если loss улучшился\n",
    "    if avg_loss <= best_loss:\n",
    "        best_loss = avg_loss\n",
    "        torch.save(model.state_dict(), \"clip.pt\")\n",
    "        print(\"Model Saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
